%
% File acl2019.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Considering the Effect of Data Source on LSTM Text Generation}

\author{Timothy Day \\
  Universitaet Tuebingen \\
  \texttt{timmday.info@gmail.com} \\}

\date{April 2019}

\begin{document}
\maketitle
\begin{abstract}

\end{abstract}


\section{Introduction}
  The fidelity of an LSTM text generation model depends critically on the quality of the data that is used to train that model.
  However, the success of text generation as a machine learning task is notoriously difficult to evaluate. What measures can we use, outside of human judgement, to decide if a character based model is generating text effectively? Is it inventing words that are complete nonsense, actual words, or words that do not exist in the training data but could sound plausible in everyday human speech? How can we tell if contrived sentences, while perhaps not strictly grammatical, would not seem out of place to language users on a medium such as Twitter vs a medium such as Wikipedia?
  
  The measure of perplexity is commonly used, but is of little practical value due to the creative (rather than predictive) nature of text generation.
  
  This paper will seek to consider the differences between texts generated by a standard LSTM model trained on data from both Wikipedia and Twitter, and will further consider Twitter data collected separately from London and Melbourne.
  
  
 \section{The Data}
The data were collected from the following sources:

\subsection{Wikipedia entry summaries} 
Summaries of Wikipedia entries were chosen at random, filtered for English language and collected using the wikipedia API on 2019.04.09.
Please see corpus\_wiki/wiki\_crawler.py in the code appendix for more information.

\subsection{Twitter posts}
These were all filtered for English language and collected using the Twitter API via tweepy. The two twitter data sets were further filtered by Geolocation to be centered around London, England on 2019.04.04 and Melbourne, Australia on 2019.04.15.
More information about how to use the tweepy package to access the twitter API using python can be found in the code appendix, in corpus\_tweets/tweepy\_crawler.py and corpus\_tweets/download\_tweets.py

\begin{table}[t!]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \textbf{Data from} & \textbf{Chars} & \textbf{Words} \\ \hline
Wiki summaries 2019.04.09 & 324527 & ~52,000 \\
Twitter London 2019.04.04 & 170067 & ~30,000 \\
Twitter Melbourne 2019.04.15 & 167917 & ~30,000\\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Corpus data source }
\end{table}



\subsection{The Corpora and the Dictionary}
Two english dictionaries were trialled for this project:
- nltk words corpus (https://www.nltk.org/api/nltk.corpus.html#module-nltk.corpus)
- pyEnchant (https://pypi.org/project/pyenchant/)

For this project, pyEnchant seemed superior for verifying modern words, of which there are many in the online corpora. It also provides dictionaries for American, British and Australia english, which is relevant for this project.




TODO corpora stats table




The following table shows the 

\begin{table}[t!]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \textbf{Corpus} & \textbf{validity rate}\\ \hline
Wiki summaries & 92.45 \\
Twitter London & 94.38 \\
Twitter Melbourne & 92.66\\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Corpora stats }
\end{table}

\subsubsection{Wikipedia Corpus Stats:}
American English dictionary used
total tokens: 62607
trash tokens (non-ACSII, punctuation chunks, numbers such as dates): 12694
total minus trash: 49913
valid dictionary tokens: 46145
invalid dictionary tokens: 3768
validity rate: 92.45\%
gross validity rate: 73.7\%

\subsubsection{Twitter London Corpus Stats:}
British English dictionary used
total tokens: 35340
trash tokens (non-ACSII, punctuation chunks, numbers such as dates): 5751
total minus trash: 29589
valid dictionary tokens: 27924
invalid dictionary tokens: 1665
validity rate: 94.38\%
gross validity rate: 79.02\%

\subsubsection{Twitter Melbourne Corpus Stats:}
Australian English dictionary used
total tokens: 35484
trash tokens (non-ACSII, punctuation chunks, numbers such as dates): 6302
total minus trash: 29182
valid dictionary tokens: 27040
invalid dictionary tokens: 2142
validity rate: 92.66\%
gross validity rate: 76.2\%


As an aside, all corpora were shown to be zipfian when word frequencies were plotted using MatPlotLib.


I must admit to being surprised with these results...
Given the level of language and ideas commonly associated with Twitter, and given how Wikipedia is relied on by society as a source of information, I expected twitter to have a much lower percentage of dictionary-valid words than Wikipedia. I expected this to be due to common spelling errors.
This was not the case, with all three corpora having roughly the same rate of dictionary-valid words (though as an Autralian I am disappointed in Melbourne for coming in last place).

However, when you consider the nature of the mediums, one can think of reasons why this could occur:
- Twitter is used for casual communication, usually about popular topics, to a mass audience. As such, it would make sense that frequently used and well understood words would comprise a higher percentage of the Twitter corpora.
- Wikipedia articles cover a range of specific and sometimes obscure topics. It is perhaps likely that a large chunk of the Wikiepdia corpus is comprised of subject-matter specific jargon that does not necessarily exist in the dictionary that the corpus was checked against.
- So it is entirely possible that Twitter indeed does contain more 'typos' than Wikipedia, but due to that nature of the language used of both mediums this is not reflected in the results.
- As an Australian, I can attest that 'Australianifying words' is a common part of the culture (example: 'Chippie' means carpenter, 'arvo' means afternoon). This is even more common for frequently used words, which are shortened. I surmise that Australians are just extra innovative with language and are inventing words faster than the disctionary can record them.



\section{The Model}



TODO elaborate a ittle here...



This paper used a standard LSTM model for character-based text generation. It is very similar to the default model described in the keras documentation.

As an LSTM, it is computationally intensive, especially for my laptop, which is why I kept the corpora relatively small and only trained for 60 Epochs. The generated output would have definitely been improved by training with more Epochs and especially more data. The model seemed to learn how to use whitespace between word


The character-based model uses a keras Sequential model, with an LSTM layer and a Dense layer for the output with a softmax activation function. It uses a root mean squared optimiser with a learning rate of 0.1.  and categorical cross entropy for the loss function.
Training went for only 60 Epochs, and text was generated for diversities of 0.2, 0.5, 1.0 and 1.2





\section{The Generated Text}
Observing the outputs of the model for the various datasets proved to be quite amusing. I have included some notable quotes.



TODO pick out the funniest quote for each




Wikipedia:
""
""
""

Twitter London:

"pad ree and twhot ling ab hall tho luke. as wine havan n illegal in brunei but today the tiny suireâ€”olk son"

"so sere tarking a brexiteer share. and stanks."

"i could to his tark to see the that is the proble the brexites that the best an wear speaking to the proble that the brexiteer libuation"

Twitter Melbourne:

"the species of the state of the commission of the mars of the south and the state of the cantons, and the county, and the conted to the careed"

""
""



As you can see above, even the cherry-picked output is largely non-sensical. However, consider the data sources. Wikipedia uses more formal and correct english by convention, and the results at first glance bear this out.
The raw twitter data however, is not at all well spelled. It often contains words that are slang, mispellings or inventions that can not be found in a dictionary.
The generated text from the twitter models seems to reflect this. It is much more chaotic than the wikipedia data, but if I read the London output with an imagined cockney accent, the output seems to make a little more sense on a phonetic level.


\section{Evaluation Metric}

The generated text can be filtered to choose a subset of it.
It can be filtered by 


\subsection{Valid Creativity}



\subsection{The Metric}

\textbf{Perplexity}: Two to the power of the categorical cross entropy (the loss function in these models) gives the perplexity, which is a metric sometimes cited alongside text generation language models.
The model produced the following perplexities:

\begin{table}[t!]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \textbf{Generation Model} & \textbf{Loss} & \textbf{Perplexity} \\ \hline
Wikipedia & 1.3045 & 2.4699 \\
Twitter London & 0.9909 & 1.9874 \\
Twitter Melbourne & 0.7896 & 1.7286 \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Perplexity Metric }
\end{table}

This has its drawbacks. What does perplexity, which is concerned with the probabilities of a sequence to predict the next part of a sequence, have to do with generated text which we are hoping will contain unique sequences? Perplexity does not consider meaning, or directly consider sentence structure. It is purely a statistical measure.




The goal here is to see if it is possible to get an idea of the 'valid creativity' of the model when it comes to generating text.

When we consider the LSTM generated text as humans, there are a few obvious factors that we automatically use to judge it:
- Do we recognise the generated words, or if not, can we at least imagine what they might mean?
- Does the sentence itself have some kind of familiar grammatical structure?
- Do the word sequences have some kind of semantic structure?

For most of the generated text, the answer to most of the above questions is a resounding no.
As such, this paper will focus solely on the individual words, and whether they appear in the English dictionary or not.
This is of course not a perfect or even adequate way of assessing the generated text, but perhaps it could act as one part of an overall assessment algorithm.


How many words can this model produce that are present in a modern dictionary, but that are not present in the training data?
Is the model able to produce plausible sounding words?



10 groups of 100 sentences were extracted at random from each of the generated texts. For each word, the following was recorded:
- does the word pass a simple spell checker test?
- did the word appear in the training data set?

To measure 'valid word creativity', the average number of dictionary-valid words that did NOT occur in the training set was recorded.

This metric has obvious limitations, a major one being that the training set itself contains many words that do not pass a standard dictionary spell check.

The results for the proportion of dictionary-valid words in each of the training sets was as follows:

The 'readability', as measured by ------ 

The code can be viewed at ( TODO )



- Dictionary tool

\subsection{Readability Analysis}

\subsection{Wug Word Generation}


\section{The Data}













\subsection{Electronically-available resources}


\subsection{Format of Electronic Manuscript}
\label{sect:pdf}
 and/or \texttt{pdflatex} which would make it easier for some.

\section{Layout}

\begin{itemize}
\item Left and right margins: 2.5 cm
\item Top margin: 2.5 cm
\item Bottom margin: 2.5 cm
\item Column width: 7.7 cm
\item Column height: 24.7 cm
\item Gap between columns: 0.6 cm
\end{itemize}

\noindent Papers should not be submitted on any other paper size.
 If you cannot meet the above requirements about the production of 
 your electronic submission, please contact the publication chairs 
 above as soon as possible.

\subsection{Fonts}

For reasons of uniformity, Adobe's \textbf{Times Roman} font should be
used. In \LaTeX2e{} this is accomplished by putting

\begin{quote}
\begin{verbatim}
\usepackage{times}
\usepackage{latexsym}
\end{verbatim}
\end{quote}
in the preamble. If Times Roman is unavailable, use \textbf{Computer
  Modern Roman} (\LaTeX2e{}'s default).  Note that the latter is about
  10\% less dense than Adobe's Times Roman font.

\begin{table}[t!]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \textbf{Type of Text} & \textbf{Font Size} & \textbf{Style} \\ \hline
paper title & 15 pt & bold \\
author names & 12 pt & bold \\
author affiliation & 12 pt & \\
the word ``Abstract'' & 12 pt & bold \\
section titles & 12 pt & bold \\
subsection titles & 11 pt & bold \\
document text & 11 pt  &\\
captions & 10 pt & \\
abstract text & 10 pt & \\
bibliography & 10 pt & \\
footnotes & 9 pt & \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Font guide. }
\end{table}

\subsection{The First Page}
\label{ssec:first}

\textbf{Title}: Place the title centered at the top of the first page, in
a 15-point bold font. (For a complete guide to font sizes and styles,
see Table~\ref{font-table}) Long titles should be typed on two lines

\textbf{Abstract}: Type the abstract at the beginning of the first
column. The width of the abstract text should be smaller than the
width of the columns for the text in the body of the paper by about
0.6 cm on each side. Center the word \textbf{Abstract} in a 12 point bold
font above the body of the abstract. The abstract should be a concise
summary of the general thesis and conclusions of the paper. It should
be no longer than 200 words. The abstract text should be in 10 point font.

\textbf{Text}: Begin typing the main body of the text immediately after
the abstract, observing the two-column format as shown in the present document. Do not include page numbers.

\textbf{Indent}: Indent when starting a new paragraph, about 0.4 cm. Use 11 points for text and subsection headings, 12 points for section headings and 15 points for the title. 


\begin{table}
\centering
\small
\begin{tabular}{cc}
\begin{tabular}{|l|l|}
\hline
\textbf{Command} & \textbf{Output}\\\hline
\verb|{\"a}| & {\"a} \\
\verb|{\^e}| & {\^e} \\
\verb|{\`i}| & {\`i} \\ 
\verb|{\.I}| & {\.I} \\ 
\verb|{\o}| & {\o} \\
\verb|{\'u}| & {\'u}  \\ 
\verb|{\aa}| & {\aa}  \\\hline
\end{tabular} & 
\begin{tabular}{|l|l|}
\hline
\textbf{Command} & \textbf{Output}\\\hline
\verb|{\c c}| & {\c c} \\ 
\verb|{\u g}| & {\u g} \\ 
\verb|{\l}| & {\l} \\ 
\verb|{\~n}| & {\~n} \\ 
\verb|{\H o}| & {\H o} \\ 
\verb|{\v r}| & {\v r} \\ 
\verb|{\ss}| & {\ss} \\\hline
\end{tabular}
\end{tabular}
\caption{Example commands for accented characters, to be used in, \emph{e.g.}, \BibTeX\ names.}\label{tab:accents}
\end{table}

\subsection{Sections}

\textbf{Headings}: Type and label section and subsection headings in the
style shown on the present document.  Use numbered sections (Arabic
numerals) in order to facilitate cross references. Number subsections
with the section number and the subsection number separated by a dot,
in Arabic numerals.
Do not number subsubsections.

\begin{table*}[t!]
\centering
\begin{tabular}{lll}
  output & natbib & previous ACL style files\\
  \hline
  \citep{Gusfield:97} & \verb|\citep| & \verb|\cite| \\
  \citet{Gusfield:97} & \verb|\citet| & \verb|\newcite| \\
  \citeyearpar{Gusfield:97} & \verb|\citeyearpar| & \verb|\shortcite| \\
\end{tabular}
\caption{Citation commands supported by the style file.
  The citation style is based on the natbib package and
  supports all natbib citation commands.
  It also supports commands defined in previous ACL style files
  for compatibility.
  }
\end{table*}

\textbf{Citations}: Citations within the text appear in parentheses
as~\cite{Gusfield:97} or, if the author's name appears in the text
itself, as Gusfield~\shortcite{Gusfield:97}.
Using the provided \LaTeX\ style, the former is accomplished using
{\small\verb|\cite|} and the latter with {\small\verb|\shortcite|} or {\small\verb|\newcite|}. Collapse multiple citations as in~\cite{Gusfield:97,Aho:72}; this is accomplished with the provided style using commas within the {\small\verb|\cite|} command, \emph{e.g.}, {\small\verb|\cite{Gusfield:97,Aho:72}|}. Append lowercase letters to the year in cases of ambiguities.  
 Treat double authors as
in~\cite{Aho:72}, but write as in~\cite{Chandra:81} when more than two
authors are involved. Collapse multiple citations as
in~\cite{Gusfield:97,Aho:72}. Also refrain from using full citations
as sentence constituents.

We suggest that instead of
\begin{quote}
  ``\cite{Gusfield:97} showed that ...''
\end{quote}
you use
\begin{quote}
``Gusfield \shortcite{Gusfield:97}   showed that ...''
\end{quote}

If you are using the provided \LaTeX{} and Bib\TeX{} style files, you
can use the command \verb|\citet| (cite in text)
to get ``author (year)'' citations.

You can use the command \verb|\citealp| (alternative cite without 
parentheses) to get ``author year'' citations (which is useful for 
using citations within parentheses, as in \citealp{Gusfield:97}).

If the Bib\TeX{} file contains DOI fields, the paper
title in the references section will appear as a hyperlink
to the DOI, using the hyperref \LaTeX{} package.
To disable the hyperref package, load the style file
with the \verb|nohyperref| option: \\{\small
\verb|\usepackage[nohyperref]{acl2019}|}

\textbf{Compilation Issues}: Some of you might encounter the following error during compilation: 

``{\em \verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.}''

This happens when \verb|pdflatex| is used and a citation splits across a page boundary. To fix this, the style file contains a patch consisting of the following two lines: (1) \verb|\RequirePackage{etoolbox}| (line 454 in \texttt{acl2019.sty}), and (2) A long line below (line 455 in \texttt{acl2019.sty}).

If you still encounter compilation issues even with the patch enabled, disable the patch by commenting the two lines, and then disable the \verb|hyperref| package (see above), recompile and see the problematic citation.
Next rewrite that sentence containing the citation. (See, {\em e.g.}, {\small\tt http://tug.org/errors.html})

\textbf{Digital Object Identifiers}:  As part of our work to make ACL
materials more widely used and cited outside of our discipline, ACL
has registered as a CrossRef member, as a registrant of Digital Object
Identifiers (DOIs), the standard for registering permanent URNs for
referencing scholarly materials.  As of 2017, we are requiring all
camera-ready references to contain the appropriate DOIs (or as a
second resort, the hyperlinked ACL Anthology Identifier) to all cited
works.  Thus, please ensure that you use Bib\TeX\ records that contain
DOI or URLs for any of the ACL materials that you reference.
Appropriate records should be found for most materials in the current
ACL Anthology at \url{http://aclanthology.info/}.

As examples, we cite \cite{P16-1001} to show you how papers with a DOI
will appear in the bibliography.  We cite \cite{C14-1001} to show how
papers without a DOI but with an ACL Anthology Identifier will appear
in the bibliography.  

As reviewing will be double-blind, the submitted version of the papers
should not include the authors' names and affiliations. Furthermore,
self-references that reveal the author's identity, \emph{e.g.},
\begin{quote}
``We previously showed \cite{Gusfield:97} ...''  
\end{quote}
should be avoided. Instead, use citations such as 
\begin{quote}
``\citeauthor{Gusfield:97} \shortcite{Gusfield:97}
previously showed ... ''
\end{quote}

Any preliminary non-archival versions of submitted papers should be listed in the submission form but not in the review version of the paper. ACL 2019 reviewers are generally aware that authors may present preliminary versions of their work in other venues, but will not be provided the list of previous presentations from the submission form. 


\textbf{Please do not use anonymous citations} and do not include
 when submitting your papers. Papers that do not
conform to these requirements may be rejected without review.

\textbf{References}: Gather the full set of references together under
the heading \textbf{References}; place the section before any Appendices. 
Arrange the references alphabetically
by first author, rather than by order of occurrence in the text.
By using a .bib file, as in this template, this will be automatically 
handled for you. See the \verb|\bibliography| commands near the end for more.

Provide as complete a citation as possible, using a consistent format,
such as the one for \emph{Computational Linguistics\/} or the one in the 
\emph{Publication Manual of the American 
Psychological Association\/}~\cite{APA:83}. Use of full names for
authors rather than initials is preferred. A list of abbreviations
for common computer science journals can be found in the ACM 
\emph{Computing Reviews\/}~\cite{ACM:83}.

The \LaTeX{} and Bib\TeX{} style files provided roughly fit the
American Psychological Association format, allowing regular citations, 
short citations and multiple citations as described above.  

\begin{itemize}
\item Example citing an arxiv paper: \cite{rasooli-tetrault-2015}. 
\item Example article in journal citation: \cite{Ando2005}.
\item Example article in proceedings, with location: \cite{borsch2011}.
\item Example article in proceedings, without location: \cite{andrew2007scalable}.
\end{itemize}
See corresponding .bib file for further details.

Submissions should accurately reference prior and related work, including code and data. If a piece of prior work appeared in multiple venues, the version that appeared in a refereed, archival venue should be referenced. If multiple versions of a piece of prior work exist, the one used by the authors should be referenced. Authors should not rely on automated citation indices to provide accurate references for prior and related work.

\textbf{Appendices}: Appendices, if any, directly follow the text and the
references (but see above).  Letter them in sequence and provide an
informative title: \textbf{Appendix A. Title of Appendix}.

\subsection{Footnotes}

\textbf{Footnotes}: Put footnotes at the bottom of the page and use 9
point font. They may be numbered or referred to by asterisks or other
symbols.\footnote{This is how a footnote should appear.} Footnotes
should be separated from the text by a line.\footnote{Note the line
separating the footnotes from the text.}

\subsection{Graphics}

\textbf{Illustrations}: Place figures, tables, and photographs in the
paper near where they are first discussed, rather than at the end, if
possible.  Wide illustrations may run across both columns.  Color
illustrations are discouraged, unless you have verified that  
they will be understandable when printed in black ink.

\textbf{Captions}: Provide a caption for every illustration; number each one
sequentially in the form:  ``Figure 1. Caption of the Figure.'' ``Table 1.
Caption of the Table.''  Type the captions of the figures and 
tables below the body, using 10 point text. Captions should be placed below illustrations. Captions that are one line are centered (see Table~\ref{font-table}). Captions longer than one line are left-aligned (see Table~\ref{tab:accents}). Do not overwrite the default caption sizes. The acl2019.sty file is compatible with the caption and subcaption packages; do not add optional arguments.



\section{Length of Submission}
\label{sec:length}

. Refer to Appendix~\ref{sec:appendix} and Appendix~\ref{sec:supplemental} for further information. 



\noindent \textbf{Preparing References:} \\
Include your own bib file like this:
\verb|\bibliographystyle{acl_natbib}|
\verb|\bibliography{acl2019}| 

where \verb|acl2019| corresponds to a acl2019.bib file.
\bibliography{acl2019}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendices}
\label{sec:appendix}
Appendices should be \textbf{uploaded as supplementary material} when submitting the paper for review. Upon acceptance, the 


\section{Supplemental Material}
\label{sec:supplemental}

\end{document}

