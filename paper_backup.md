
\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{tabularx}
\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Creative Validity: Evaluating Text Generation Models with varied Data Sources}

\author{Timothy Day \\
  Universitaet Tuebingen \\
  \texttt{timmday.info@gmail.com} \\}

\date{April 2019}

\begin{document}
\maketitle


\begin{abstract}
The fidelity of a Text Generation model depends critically on the quality of the data that is used to train that model.
  However, the success of text generation as a machine learning task is notoriously difficult to evaluate. 
  
  The measure of perplexity is commonly used, but is of little practical value due to the creative (rather than purely predictive) nature of the text generation task.
  
  This paper will seek to consider the 'creativity' of text generated by three standard \textbf{character-based LSTM} models, trained on data from  Wikipedia, London Twitter and Melbourne Twitter.
  We will evaluate the generated text, to find how many generated tokens appear in a standard English dictionary but NOT in the corpus that the model was trained on. These tokens - unseen valid dictionary words - will be referred to as 'creatively valid' tokens.
\end{abstract}



\section{Introduction}
  What metrics can we use, outside of human judgement, to decide if a character based model is generating text effectively? Is it inventing tokens that are complete nonsense, or actual words, or tokens that sound like they could be words? How \textbf{creative} is the model, while still being legible?
  
  It is on the creativity of the model that this paper will focus. As a sequential model, a LSTM network is predictive in nature, and hence it will tend to produce output similar to what it has already seen in the training data. However, if the model can learn common syllables, character groupings and also what characters should not go together it can produce tokens that are not only novel but can also be found in a modern dictionary.
  
  This paper will measure the rate of production of dictionary-valid tokens that are not in the training data, for models trained on the three different sets of training data.
  


\section{The Data}

\subsection{Wikipedia entry summaries} 
Summaries of Wikipedia entries were chosen at random, filtered for English language and collected using the wikipedia API on 2019.04.09.
Please see 
\begin{quote}
\begin{verbatim}
corpus_wiki/wiki_crawler.py
\end{verbatim}
\end{quote}
 in the code appendix for more information.

\subsection{Twitter posts}
Posts were filtered for English language and collected using the Twitter API via tweepy. The two twitter data sets were further filtered by Geolocation to be centered around London, England on 2019.04.04 and Melbourne, Australia on 2019.04.15.
More information about how to use the tweepy package to access the twitter API using python can be found in the code appendix, in
\begin{quote}
\begin{verbatim}
corpus_tweets/tweepy_crawler.py
\end{verbatim}
\end{quote}
\begin{quote}
\begin{verbatim}
corpus_tweets/download_tweets.py
\end{verbatim}
\end{quote}


\begin{table}[h!]
% \begin{center}
\begin{tabular}{|l|rl|}
\hline \textbf{Data from} & \textbf{Chars} & \textbf{Tokens} \\ \hline
Wiki summaries 2019.04.09 & 324527 & ~52,000 \\
Twitter London 2019.04.04 & 170067 & ~30,000 \\
Twitter Melbourne 2019.04.15 & 167917 & ~30,000\\
\hline
\end{tabular}
% \end{center}
\caption{\label{font-table} Corpora data sources }
\end{table}




\section{Corpus Evaluation}
\subsection{The English Dictionary}
The corpora were evaluated by checking the tokens against a relevant English dictionary.
Two english dictionaries were trialled for this project:
\begin{itemize}
\item NLTK words corpus \footnote{https://www.nltk.org/api/nltk.corpus.html#module-nltk.corpus}

\item pyEnchant
\footnote{https://pypi.org/project/pyenchant/}

\end{itemize}
PyEnchant seemed superior for verifying modern slang-like words, of which there are many in the online corpora. It also provides dictionaries for American, British and Australian English, which is relevant for this project.

The following table shows the percentage of tokens in each of the corpora that appear in the pyEnchant dictionary. Before calculating this percentage, the corpora were first stripped of tokens that were purely punctuation, contained numbers or contained non-ASCII characters.

\begin{table}[h!]
% \begin{center}
\begin{tabular}{|l|c|l|}
\hline \textbf{Corpus} & \textbf{validity rate}\\ \hline
Wiki summaries & 92.45\% \\
Twitter London & 94.38\% \\
Twitter Melbourne & 92.66\% \\
\hline
\end{tabular}
% \end{center}
\caption{\label{font-table} Corpora stats }
\end{table}


As an aside, all corpora were shown to be zipfian when word frequencies were plotted using MatPlotLib.

I must admit to feeling surprised that Twitter did not have a lower percentage of dictionary-valid words than Wikipedia. I assumed that Twitter would have many more spelling errors, and suspected that the nature of the medium (argumentative and emotional) compared to the nature of Wikipedia (encyclopedia-like) would lead to more emotional, inventive and non-dictionary valid language on Twitter.


When you consider the nature of the mediums, one can think of more reasons to explain this:


\begin{itemize}
\item  Twitter is used for casual communication, usually about popular topics, to a mass audience. As such, it would make sense that frequently used and well understood words would comprise a higher percentage of the Twitter corpora.
\item  Wikipedia articles cover a range of specific and sometimes obscure topics. It is perhaps likely that a large chunk of the Wikiepdia corpus is comprised of subject-matter specific jargon that does not necessarily exist in the dictionary that the corpus was checked against.
\item tokens containing non-ASCII characters were not considered. So this excludes the emojis which are relatively comon in twitter posts.
\item  It is entirely possible that Twitter indeed does contain more 'typos' than Wikipedia, but due to that nature of the language used of both mediums this is not reflected in the results.
\item  As an Australian, I can attest that 'Australianifying' words is a common part of the culture (example: 'Chippie' means carpenter, 'arvo' means afternoon). This is even more common for frequently used words, which are shortened. I surmise that the reason the Australian twitter corpus had the least valid tokens is that Australians are inventing words faster than the dictionary can record them. Most Australians have no respect for dictionaries, and forge their own path through life.
\end{itemize}




\section{The Model}

This paper used a standard LSTM model for character-based text generation. It is similar to the default model described in the keras documentation \footnote{ https://keras.io/layers/recurrent/#lstm}.

As an LSTM, it is computationally intensive, especially for my laptop, which is why I kept the corpora relatively small and only trained for 60 Epochs. The generated output would have been improved by training with more Epochs and especially more data, however the model still achieved some decent results for our purposes. For example, it recognised how to use whitespace between words, basic punctuation and what character combinations make up the most common syllables. This allows us to view the generation of creatively valid tokens.


The model uses a keras Sequential model, with an LSTM layer with an output space of dimensionality 128, and a Dense layer with a softmax activation function. It uses a root mean squared optimiser with a learning rate of 0.1, with categorical cross entropy for the loss function.
Training went for 60 Epochs, and text was generated for diversities of 0.2, 0.5, 1.0 and 1.2.

The code can be found in the code appendix, in train\_text\_gen\_char.py

Some ideas for improving the model include:
\begin{itemize}
    \item triple or quadruple the size of the training corpora
    \item increase the output dimensionality from 128 to 256
    \item add a dropout layer after the LSTM layer to prevent overfitting
    \item add another LSTM layer
    \item tune the learning rate
    \item tune the dropout layer(s)
    \item decrease the batch size from 128
    \item increase the number of epochs
    \item pre-train the model on some 'proof read' English data, such as a series of novels.
\end{itemize}
And definitely train on GPU hardware!







\section{The Generated Text}
The generated text was mostly nonsensical but was arguably, and amusingly, reflective of the source data. For example, the London model was overly concerned with Brexit.

\begin{quote}
\textit{so sere tarking a brexiteer share. and stanks.}
\end{quote}

\begin{quote}
\textit{i could to his tark to see the that is the proble the brexites that the best an wear speaking to the proble that the brexiteer libuation"}
\end{quote}

... and would sometimes sound a lot like a rambling drunk Englishman you might encounter on a London street:

\begin{quote}
\textit{pad ree and twhot ling ab hall tho luke. as wine havan n illegal in brunei but today the tiny suireâ€”olk son}
\end{quote}

Whereas the Melbourne model sounded less British, but equally drunk. It would sometimes show some Australian snark:

\begin{quote}
\textit{people who like astrology: i like astrology what}
\end{quote}
\begin{quote}
\textit{pashing! men in open morally liket seems just my mini now gurf!}
\end{quote}

I had hoped that Wikipedia would bring a higher tone, but alas, it fell in love with prepositional phrases (which seems like an Encyclopedic thing to do):

\begin{quote}
\textit{in marrian and har brewery and the university of commission of the commended as the competition of the southern public of the commonly in the southern purchum of the province of the commission of the communitial can extene.}
\end{quote}

The Encyclopedic tone did shine through the jibberish, and phrases did seem to make a little more sense than in the generated text from Twitter. This was likely because the Wikipedia dataset was larger.

\begin{quote}
\textit{the hugal capital attackered to matchuffered to dwate in 1984.}
\end{quote}
\begin{quote}
\textit{the people of the fail competition of the australia}
\end{quote}
\begin{quote}
\textit{a dvd featuring a selection of eleven musicia oillially moroped}
\end{quote}



\section{Evaluation Metric: Obtaining the Valid Creativity}

\subsection{Why not Perplexity?}
\textbf{Perplexity}: Two to the power of the categorical cross entropy (the loss function in these models) gives the perplexity, which is a metric sometimes cited alongside text generation language models.
The model produced the following perplexities:

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \textbf{Generation Model} & \textbf{Loss} & \textbf{Perplexity} \\ \hline
Wikipedia & 1.3045 & 2.4699 \\
Twitter London & 0.9909 & 1.9874 \\
Twitter Melbourne & 0.7896 & 1.7286 \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Perplexity Metric }
\end{table}

As a purely statistical metric, perplexity does not directly consider meaning or sentence structure. Most importantly, as a measure of the predictive power of a sequence, it does not deal well with measuring creativity - ie. when the model produces a novel word that is valid, but is not present in the training set.

For the task of text generation, novel but valid tokens are a good thing! This is why measuring 'valid creativity' might be useful to help us evaluate text generation models.

\subsection{Valid Creativity}
For the purposes of this paper, we will define \textit{valid creativity} as:

the number of tokens that are generated by a model, that are present in a standard English dictionary but NOT present in the training corpus, divided by the total number of valid tokens in the generated text.
Valid tokens in the generated text are defined as tokens that do not contain numbers, are not exclusively made up of punctuation and do not contain non-ASCII characters (a cheap way of filtering out foreign language proper nouns). 

This metric has obvious limitations, a major one being that the training set itself contains many words that do not pass a standard dictionary spell check. The quality of the dictionary used for checking will also have a big effect on the results. With this in mind, this paper attempted to match the various corpora with dialect specific versions of the English dictionary.


\subsubsection{Calculating Valid Creativity}

The generated text was filtered to only select text generated after epoch 50.  It was further filtered for each of the four diversities. At the evaluation step, any tokens that were purely punctuation or contained numbers were filtered out.

There are 12 sets of generated text to evaluate (3 models * 4 diversities). 
For each set, the following thing were tallied:
\begin{itemize}
    \item the total number of tokens
    \item generated tokens that appeared in the training data
    \item generated tokens that passed the dictionary test
    \item generated tokens that passed the dictionary test but DID NOT appear in the training data ( creatively valid)
\end{itemize}

The code can be seen in the code appendix:
\begin{quote}
\begin{verbatim}
process_gen_text.py
\end{verbatim}
\end{quote}

The results for the proportion of dictionary-valid words in each of the training sets are outlined in the following tables:


\begin{table*}[h]
\centering
\begin{tabular}{l|l|l|l}
  \textbf{DIVERSITY 0.2} & London Twitter & Melbourne Twitter & Wikipedia\\
  \hline
  Total Tokens & 784 & 928 & 887\\
  Dictionary & En\_GB & En\_AU & En\_US\\
  Dict Pass & 608 & 709 & 806\\
  Corpus Pass & 623 & 733 & 805\\
  Creatively Valid & 16 & 27 & 10\\
  Creative Validity Rate (\%) & 2.04 & 2.91 & 1.13\\
\end{tabular}
\caption{evaluation of generated text with diversity 0.2}
\end{table*}


\begin{table*}[h]
\centering
\begin{tabular}{l|l|l|l}
  \textbf{DIVERSITY 0.5} & London Twitter & Melbourne Twitter & Wikipedia\\
  \hline
  Total Tokens & 863 & 837 & 858\\
  Dictionary & En\_GB & En\_AU & En\_US\\
  Dict Pass & 654 & 615 & 731\\
  Corpus Pass & 665 & 644 & 728\\
  Creatively Valid & 20 & 16 & 20\\
  Creative Validity Rate (\%) & 2.32 & 1.91 & 2.33\\
\end{tabular}
\caption{evaluation of generated text with diversity 0.5}
\end{table*}


\begin{table*}[h]
\centering
\begin{tabular}{l|l|l|l}
  \textbf{DIVERSITY 1.0} & London Twitter & Melbourne Twitter & Wikipedia\\
  \hline
  Total Tokens & 779 & 814 & 755\\
  Dictionary & En\_GB & En\_AU & En\_US\\
  Dict Pass & 507 & 522 & 477\\
  Corpus Pass & 504 & 531 & 481\\
  Creatively Valid & 26 & 25 & 17\\
  Creative Validity Rate & 3.34 & 3.07 & 2.25\\
\end{tabular}
\caption{evaluation of generated text with diversity 1.0}
\end{table*}


\begin{table*}[h]
\centering
\begin{tabular}{l|l|l|l}
  \textbf{DIVERSITY 1.2} & London Twitter & Melbourne Twitter & Wikipedia\\
  \hline
  Total Tokens & 759 & 745 & 691\\
  Dictionary & En\_GB & En\_AU & En\_US\\
  Dict Pass & 451 & 442 & 368\\
  Corpus Pass & 445 & 432 & 354\\
  Creatively Valid & 30 & 35 & 28\\
  Creative Validity Rate & 3.95 & 4.70 & 4.05\\
\end{tabular}
\caption{evaluation of generated text with diversity 1.2}
\end{table*}


\subsection{Discussion of Results}
It is not surprising that as diversity increased, so did valid creativity, while the total number of tokens passing a dictionary test decreased. A model with higher diversity is going to try crazier things, and hence have more dictionary misses in total but also discover more novel words.

While it is interesting to observe the variety of valid creativity rates across diversity, there appears to be no significant differences across the three data sources.

The differences between the data sources are more apparent on a subjective level, considering what the Creatively Valid tokens actually were.

For example, at a diversity of 1.0, London Twitter invented some quite British sounding words including:
twill, drear, eave, ho.

The Australian twitter model invented some Aussie sounding words:
shout (to buy a round of drinks), dons (a Melbourne football team nickname), wined (to be served wine), ton (Australian spelling of tonne)

The Wikipedia model invented the longest creatively valid token with "socialization" with 13 characters. Notably, the training corpus included several instances of 'social' and many instances of words ending in 'ization'.
The longest valid token from the London model was 'righting' with 8 characters, while the Australia model produced 'morally' with 7 characters.
Notably, the longest words produced by all models were made up of component syllables that the model had tried joining together.


\section{Supplemental Material}
Please find the code used for this project on github
\begin{verbatim}
https://github.com/TimmDay/
ml-lstm-evauluating-text-gen
\end{verbatim}

\end{document}



